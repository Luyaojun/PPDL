{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "from __future__ import division\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define constants\n",
    "NUM_TRAINING_IMAGES = 60000\n",
    "NUM_TESTING_IMAGES = 10000\n",
    "IMAGE_SIZE = 28\n",
    "mnist_train_file = '/home/yang/Research/Privacy-preserving-DL/PPDL/DATA/MNIST/mnist_train.tfrecord'\n",
    "mnist_test_file = '/home/yang/Research/Privacy-preserving-DL/PPDL/DATA/MNIST/mnist_test.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define parameters\n",
    "usrSize = np.array([4000, 2000, 1000, 6000, 3000, 2000])\n",
    "users = 6\n",
    "lotSize = 300  # the number of samples used in each iteration by each party (at most can use all of the samples in the party)\n",
    "batchSize = 100 \n",
    "epochs = 1   ## how many runs on the lot at each iteration?\n",
    "epsilon = [.05,.04,.05,.08,.03,.04] # privacy budget for each iteration\n",
    "delta = np.reciprocal(usrSize, dtype = float) * .5\n",
    "totalIter = 10\n",
    "learning_rate = 1\n",
    "grad_bound = .001  ## how to set???\n",
    "# neural network structures\n",
    "layers = 2  # number of hidden layer\n",
    "units = np.array([100,50])  # number of units in each hidden layer\n",
    "input = IMAGE_SIZE ** 2  # input size\n",
    "output = 10  # output size\n",
    "ratio = .1 # the top 10% of the gradient values will be used for the update\n",
    "paramD = input * units[0] + units[0] + units[0] * units[1] + units[1] + units[1] * output + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define functions\n",
    "def MnistInput(mnist_data_file, whole=True, start=None, size=None):\n",
    "    \"\"\"Create operations to read the MNIST input file.\n",
    "      Args:\n",
    "        mnist_data_file: Path of a tfrecord file containing the MNIST images to process.\n",
    "        whole: when set to true, return the whole MNIST dataset (training or test set)\n",
    "        start: start index of the first sample in the user dataset\n",
    "        size: size of the user dataset\n",
    "\n",
    "      Returns:\n",
    "        images: A list with the formatted image data. default shape [10000, 28*28]\n",
    "        labels: A list with the labels for each image. default shape [10000]\n",
    "      \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        file_queue = tf.train.string_input_producer([mnist_data_file], num_epochs=1)\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, value = reader.read(file_queue)\n",
    "        example = tf.parse_single_example(\n",
    "            value,\n",
    "            features={\"image/encoded\": tf.FixedLenFeature(shape=(), dtype=tf.string),\n",
    "                      \"image/class/label\": tf.FixedLenFeature([1], tf.int64)})\n",
    "\n",
    "        image = tf.cast(tf.image.decode_png(example[\"image/encoded\"], channels=1),\n",
    "                        tf.float32)\n",
    "        image = tf.reshape(image, [IMAGE_SIZE * IMAGE_SIZE])\n",
    "        image /= 255\n",
    "        label = tf.cast(example[\"image/class/label\"], dtype=tf.int32)\n",
    "        label = tf.reshape(label, [])\n",
    "\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        init_op2 = tf.local_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        sess.run(init_op2)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        images = []\n",
    "        labels = []\n",
    "        if whole:\n",
    "            try:\n",
    "                while True:\n",
    "                    i, l = sess.run([image, label])\n",
    "                    i = i.tolist()\n",
    "                    images.append(i)\n",
    "                    labels.append(l)\n",
    "            except tf.errors.OutOfRangeError, e:\n",
    "                coord.request_stop(e)\n",
    "            finally:\n",
    "                coord.request_stop()\n",
    "                coord.join(threads)\n",
    "        else:\n",
    "            try:\n",
    "                for k in xrange(start - 1):\n",
    "                    sess.run([image, label])\n",
    "                for k in xrange(start, start + size):\n",
    "                    i, l = sess.run([image, label])\n",
    "                    i = i.tolist()\n",
    "                    images.append(i)\n",
    "                    labels.append(l)\n",
    "            except tf.errors.OutOfRangeError, e:\n",
    "                coord.request_stop(e)\n",
    "            finally:\n",
    "                coord.request_stop()\n",
    "                coord.join(threads)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "    \n",
    "def privParm(epsilon, iter, delta):\n",
    "    \"\"\"\n",
    "    Calculate the epsilon for each iteration based on the total privacy parameters\n",
    "    ref: DP book Corollary 3.21 (strong composition theorem)\n",
    "      Args:\n",
    "        epsilon: total privacy budget for the user\n",
    "        iter: total iteration\n",
    "        delta: total tolerable privacy failure probability \n",
    "      Returns:\n",
    "        stepEps: the privacy budget for each iteration\n",
    "    \"\"\"\n",
    "    return epsilon / (2*np.sqrt(2*iter*np.log(1/delta)))\n",
    "\n",
    "\n",
    "def trainNN(images, labels, size, learningRate, bound, Lot_size, batch_size,epochs):\n",
    "    \"\"\"\n",
    "    build and train the neural network, return the gradient and updated parameter\n",
    "        Args:\n",
    "            weight,bias: weights and bias for each layer (list of list)\n",
    "            iter: iteration number\n",
    "            images,labels: images and labels for this user\n",
    "\n",
    "        Returns:           \n",
    "            parameter (weight, bias) difference\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess: \n",
    "        #lr = tf.placeholder(tf.float32)  # learning rate        \n",
    "        # hard-coded the network structure (2 hidden layers)\n",
    "        #W1 = tf.Variable(weight[0], dtype=tf.float32)  # first hidden layer\n",
    "        #W2 = tf.Variable(weight[1], dtype=tf.float32)  # second hidden layer\n",
    "        #W3 = tf.Variable(weight[2], dtype=tf.float32)  # output layer\n",
    "        #b1 = tf.Variable(bias[0], dtype=tf.float32)\n",
    "        #b2 = tf.Variable(bias[1], dtype=tf.float32)\n",
    "        #b3 = tf.Variable(bias[2], dtype=tf.float32)\n",
    "    \n",
    "        init = tf.global_variables_initializer()\n",
    "        init_2 = tf.local_variables_initializer()\n",
    "        # initialize variables\n",
    "        sess.run([init, init_2])\n",
    "        assign_op1 = W1.assign(weight[0])\n",
    "        assign_op2 = W2.assign(weight[1])\n",
    "        assign_op3 = W3.assign(weight[2])\n",
    "        assign_op4 = b1.assign(bias[0])\n",
    "        assign_op5 = b2.assign(bias[1])\n",
    "        assign_op6 = b3.assign(bias[2])\n",
    "        sess.run([assign_op1, assign_op2, assign_op3, assign_op4, assign_op5, assign_op6]) \n",
    "        \n",
    "        \n",
    "        round = int(Lot_size / batch_size)\n",
    "        # sample lot_size samples from the user data\n",
    "        Lot_index = random.sample(range(size),Lot_size)\n",
    "        image_lot = [images[i] for i in Lot_index]\n",
    "        label_lot = [labels[i] for i in Lot_index]\n",
    "        image_lot = tf.convert_to_tensor(image_lot, dtype=tf.float32)\n",
    "        label_lot = tf.convert_to_tensor(label_lot)        \n",
    "        \n",
    "        for _ in xrange(epochs):\n",
    "            for i in xrange(round):\n",
    "                # sample one batch from the lot\n",
    "                image_batch = image_lot[(i*batch_size):((i+1)*batch_size),:]\n",
    "                label_batch = label_lot[(i*batch_size):((i+1)*batch_size)]\n",
    "                # build the network\n",
    "                Y1 = tf.nn.relu(tf.matmul(image_batch, W1) + b1)  # first layer\n",
    "                Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)  # first layer\n",
    "                Ylogits = tf.matmul(Y2, W3) + b3  # output layer\n",
    "                Y = tf.nn.softmax(Ylogits)\n",
    "                # objective function (cross entropy)\n",
    "                cost = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=tf.one_hot(label_batch, 10))\n",
    "                cost = tf.reduce_sum(cost) / batch_size\n",
    "                op1 = tf.train.GradientDescentOptimizer(lr).compute_gradients(cost)\n",
    "                op2 = tf.train.GradientDescentOptimizer(lr).apply_gradients(op1)\n",
    "                saver = tf.train.Saver()\n",
    "                coord = tf.train.Coordinator()\n",
    "                _ = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "                sess.run(op1) \n",
    "                sess.run([op2], feed_dict={lr: learningRate})\n",
    "\n",
    "        newW1 = sess.run(W1)\n",
    "        newW2 = sess.run(W2)\n",
    "        newW3 = sess.run(W3)\n",
    "        newb1 = sess.run(b1)\n",
    "        newb2 = sess.run(b2)\n",
    "        newb3 = sess.run(b3)\n",
    "    #sess.close()\n",
    "\n",
    "    # clip difference\n",
    "    W1_diff = np.clip((newW1 - weight[0]).flatten(), -bound, bound)\n",
    "    W2_diff = np.clip((newW2 - weight[1]).flatten(), -bound, bound)\n",
    "    W3_diff = np.clip((newW3 - weight[2]).flatten(), -bound, bound)\n",
    "    b1_diff = np.clip((newb1 - bias[0]).flatten(), -bound, bound)\n",
    "    b2_diff = np.clip((newb2 - bias[1]).flatten(), -bound, bound)\n",
    "    b3_diff = np.clip((newb3 - bias[2]).flatten(), -bound, bound)\n",
    "    print \"one round\"\n",
    "    return W1_diff, W2_diff, W3_diff, b1_diff, b2_diff, b3_diff\n",
    "\n",
    "def noisyMax(grad, n, epsilon, bound):\n",
    "    \"\"\"\n",
    "    report noisy maximum algorithm\n",
    "            Args:\n",
    "                grad: true gradient of each parameter\n",
    "                n: # of output noisy gradient\n",
    "                scale: scale of the laplace distribution (calibrated by sensitivity)\n",
    "            Returns: \n",
    "                the noisy version of the top n gradient\n",
    "    \"\"\"\n",
    "    l = len(grad)\n",
    "    scale = 2 * bound *2 * n / epsilon  \n",
    "    noisyGrad = grad + np.random.laplace(loc = 0.0, scale = scale, size = len(grad))\n",
    "    index = sorted(range(l), key=lambda i: np.abs(noisyGrad[i]))[-int(n):]\n",
    "    \n",
    "    #return [noisyGrad[i] if i in index else 0 for i in range(l)]\n",
    "    return [0.1 * np.sign(noisyGrad[i]) if i in index else 0 for i in range(l)]\n",
    "    \n",
    "    \n",
    "def account(epsilon, iter, delta):\n",
    "    \"\"\"\n",
    "    return the total used privacy budget based on budget on each iteration\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.sqrt(2*iter*np.log(1/delta))*epsilon + iter*epsilon*(np.exp(epsilon) - 1)\n",
    "\n",
    "\n",
    "# function to evaluate the accuracy on the test set\n",
    "def eval(images,labels):\n",
    "    #tf.Graph().as_default()\n",
    "    #testimages, testlabels = MnistInput(mnist_test_file, whole=True)\n",
    "    #testimages = tf.convert_to_tensor(testimages, dtype=tf.float32)\n",
    "    #testlabels = tf.convert_to_tensor(testlabels)    \n",
    "    #tf.device('/cpu:0')\n",
    "    #lr = tf.placeholder(tf.float32)  # learning rate        \n",
    "    # hard-coded the network structure (2 hidden layers)\n",
    "    #W1 = tf.Variable(weight[0], dtype=tf.float32)  # first hidden layer\n",
    "    #W2 = tf.Variable(weight[1], dtype=tf.float32)  # second hidden layer\n",
    "    #W3 = tf.Variable(weight[2], dtype=tf.float32)  # output layer\n",
    "    #b1 = tf.Variable(bias[0], dtype=tf.float32)\n",
    "    #b2 = tf.Variable(bias[1], dtype=tf.float32)\n",
    "    #b3 = tf.Variable(bias[2], dtype=tf.float32)\n",
    "    Y1 = tf.nn.relu(tf.matmul(images, W1) + b1)  # first layer\n",
    "    Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)  # second layer\n",
    "    Ylogits = tf.matmul(Y2, W3) + b3  # output layer\n",
    "    Y = tf.nn.softmax(Ylogits)\n",
    "    \n",
    "    with tf.Session() as sess:            \n",
    "        init = tf.global_variables_initializer()\n",
    "        init_2 = tf.local_variables_initializer()\n",
    "        # initialize variables\n",
    "        sess.run([init, init_2])\n",
    "        assign_op1 = W1.assign(weight[0])\n",
    "        assign_op2 = W2.assign(weight[1])\n",
    "        assign_op3 = W3.assign(weight[2])\n",
    "        assign_op4 = b1.assign(bias[0])\n",
    "        assign_op5 = b2.assign(bias[1])\n",
    "        assign_op6 = b3.assign(bias[2])\n",
    "        sess.run([assign_op1, assign_op2, assign_op3, assign_op4, assign_op5, assign_op6])        \n",
    "        \n",
    "        aa = sess.run(tf.argmax(Y, 1))\n",
    "        cc = sess.run(labels)\n",
    "        accuracy = sum(aa == cc) / NUM_TESTING_IMAGES\n",
    "    \n",
    "    #tf.reset_default_graph() \n",
    "\n",
    "    return accuracy\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the main process\n",
    "# the center read the test images and labels\n",
    "# read test dataset\n",
    "testimages, testlabels = MnistInput(mnist_test_file, whole=True)\n",
    "testimages = tf.convert_to_tensor(testimages, dtype=tf.float32)\n",
    "testlabels = tf.convert_to_tensor(testlabels)\n",
    "# initialize parameters and iter\n",
    "iter = 0\n",
    "\n",
    "initW1 = np.zeros((input, units[0]))\n",
    "initb1 = np.zeros(units[0])\n",
    "initW2 = np.zeros((units[0], units[1]))\n",
    "initb2 = np.zeros(units[1])\n",
    "initW3 = np.zeros((units[1], output))\n",
    "initb3 = np.zeros(output)\n",
    "\n",
    "weight = [initW1,initW2,initW3]\n",
    "bias = [initb1,initb2,initb3]\n",
    "\n",
    "\n",
    "# build the network\n",
    "tf.Graph().as_default()\n",
    "tf.device('/cpu:0')\n",
    "lr = tf.placeholder(tf.float32)  # learning rate        \n",
    "# hard-coded the network structure (2 hidden layers)\n",
    "W1 = tf.Variable(weight[0], dtype=tf.float32)  # first hidden layer\n",
    "W2 = tf.Variable(weight[1], dtype=tf.float32)  # second hidden layer\n",
    "W3 = tf.Variable(weight[2], dtype=tf.float32)  # output layer\n",
    "b1 = tf.Variable(bias[0], dtype=tf.float32)\n",
    "b2 = tf.Variable(bias[1], dtype=tf.float32)\n",
    "b3 = tf.Variable(bias[2], dtype=tf.float32)    \n",
    "\n",
    "imageX = tf.placeholder(tf.float32, [None, 784], name=\"image\")\n",
    "labelY = tf.placeholder(tf.int64, [None, 10], name=\"label\")\n",
    "\n",
    "Y1 = tf.nn.relu(tf.matmul(imageX, W1) + b1)  # first layer\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)  # first layer\n",
    "Ylogits = tf.matmul(Y2, W3) + b3  # output layer\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "# objective function (cross entropy)\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=tf.one_hot(labelY, 10))\n",
    "cost = tf.reduce_sum(cost) / batchSize\n",
    "\n",
    "\n",
    "\n",
    "# record the accuracy\n",
    "accuracy = []\n",
    "accuracy.append(eval(testimages, testlabels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read user data\n",
    "images1, labels1 = MnistInput(mnist_train_file, whole=False, start=1, size = usrSize[0])\n",
    "images2, labels2 = MnistInput(mnist_train_file, whole=False, start=1 + usrSize[0], size = usrSize[1])\n",
    "images3, labels3 = MnistInput(mnist_train_file, whole=False, start=1 + usrSize[0] + usrSize[1], size = usrSize[2])\n",
    "images4, labels4 = MnistInput(mnist_train_file, whole=False, start=1 + usrSize[0] + usrSize[1] + usrSize[2], size = usrSize[3])\n",
    "images5, labels5 = MnistInput(mnist_train_file, whole=False, start=1 + usrSize[0] + usrSize[1] + usrSize[2] + usrSize[3], size = usrSize[4])\n",
    "images6, labels6 = MnistInput(mnist_train_file, whole=False, start=1 + usrSize[0] + usrSize[1] + usrSize[2] + usrSize[3] + usrSize[4], size = usrSize[5])\n",
    "\n",
    "#images1 = tf.convert_to_tensor(images1, dtype=tf.float32)\n",
    "#labels1 = tf.convert_to_tensor(labels1)\n",
    "#images2 = tf.convert_to_tensor(images2, dtype=tf.float32)\n",
    "#labels2 = tf.convert_to_tensor(labels2)\n",
    "#images3 = tf.convert_to_tensor(images3, dtype=tf.float32)\n",
    "#labels3 = tf.convert_to_tensor(labels3)\n",
    "#images4 = tf.convert_to_tensor(images4, dtype=tf.float32)\n",
    "#labels4 = tf.convert_to_tensor(labels4)\n",
    "#images5 = tf.convert_to_tensor(images5, dtype=tf.float32)\n",
    "#labels5 = tf.convert_to_tensor(labels5)\n",
    "#images6 = tf.convert_to_tensor(images6, dtype=tf.float32)\n",
    "#labels6 = tf.convert_to_tensor(labels6)\n",
    "\n",
    "trainImages = [images1,images2,images3,images4,images5,images6]\n",
    "trainLabels = [labels1,labels2,labels3,labels4,labels5,labels6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one round\n"
     ]
    }
   ],
   "source": [
    "# go through the users for iterations\n",
    "for _ in range(totalIter):\n",
    "    for i in range(users):\n",
    "        W1_diff, W2_diff, W3_diff, b1_diff, b2_diff, b3_diff = \\\n",
    "            trainNN(trainImages[i], trainLabels[i], usrSize[i], learning_rate, grad_bound, lotSize, batchSize,epochs)\n",
    "        \n",
    "        W1_diff_noisy = np.array(noisyMax(W1_diff, len(W1_diff) * ratio, epsilon[i], grad_bound)).reshape(weight[0].shape)\n",
    "        W2_diff_noisy = np.array(noisyMax(W2_diff, len(W2_diff) * ratio, epsilon[i], grad_bound)).reshape(weight[1].shape)\n",
    "        W3_diff_noisy = np.array(noisyMax(W3_diff, len(W3_diff) * ratio, epsilon[i], grad_bound)).reshape(weight[2].shape)\n",
    "        b1_diff_noisy = np.array(noisyMax(b1_diff, len(b1_diff) * ratio, epsilon[i], grad_bound)).reshape(bias[0].shape)\n",
    "        b2_diff_noisy = np.array(noisyMax(b2_diff, len(b2_diff) * ratio, epsilon[i], grad_bound)).reshape(bias[1].shape)\n",
    "        b3_diff_noisy = np.array(noisyMax(b3_diff, len(b3_diff) * ratio, epsilon[i], grad_bound)).reshape(bias[2].shape)\n",
    "        \n",
    "        # update the parameter\n",
    "        weight[0] = weight[0] + W1_diff_noisy\n",
    "        weight[1] = weight[1] + W2_diff_noisy\n",
    "        weight[2] = weight[2] + W3_diff_noisy\n",
    "        bias[0] = bias[0] + b1_diff_noisy\n",
    "        bias[1] = bias[1] + b2_diff_noisy\n",
    "        bias[2] = bias[2] + b3_diff_noisy\n",
    "                \n",
    "    accuracy.append(eval(testimages, testlabels))\n",
    "    print accuracy\n",
    "                        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################3######################################\n",
    "#########################################3######################################\n",
    "#########################################3######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the main process\n",
    "# the center read the test images and labels\n",
    "# read test dataset\n",
    "testimages, testlabels = MnistInput(mnist_test_file, whole=True)\n",
    "testimages = tf.convert_to_tensor(testimages, dtype=tf.float32)\n",
    "testlabels = tf.convert_to_tensor(testlabels)\n",
    "# initialize parameters and iter\n",
    "iter = 0\n",
    "\n",
    "initW1 = np.zeros((input, units[0]), dtype=np.float32)\n",
    "initb1 = np.zeros(units[0], dtype=np.float32)\n",
    "initW2 = np.zeros((units[0], units[1]), dtype=np.float32)\n",
    "initb2 = np.zeros(units[1], dtype=np.float32)\n",
    "initW3 = np.zeros((units[1], output), dtype=np.float32)\n",
    "initb3 = np.zeros(output, dtype=np.float32)\n",
    "\n",
    "weight = [initW1,initW2,initW3]\n",
    "bias = [initb1,initb2,initb3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'W1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5744fede2acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# record the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-73c21fa2af85>\u001b[0m in \u001b[0;36meval\u001b[0;34m(images, labels)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m#b2 = tf.Variable(bias[1], dtype=tf.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m#b3 = tf.Variable(bias[2], dtype=tf.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mY1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0mY2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# second layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mYlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb3\u001b[0m  \u001b[0;31m# output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'W1' is not defined"
     ]
    }
   ],
   "source": [
    "# record the accuracy\n",
    "accuracy = []\n",
    "accuracy.append(eval(testimages, testlabels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y1 = tf.nn.relu(tf.matmul(testimages, W1) + b1)  # first layer\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)  # second layer\n",
    "Ylogits = tf.matmul(Y2, W3) + b3  # output layer\n",
    "Y = tf.nn.softmax(Ylogits)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                \n",
    "    init = tf.global_variables_initializer()\n",
    "    init_2 = tf.local_variables_initializer()\n",
    "    # initialize variables\n",
    "    sess.run([init, init_2])\n",
    "        \n",
    "    aa = sess.run(tf.argmax(Y, 1))\n",
    "    cc = sess.run(testlabels)\n",
    "    accuracy = sum(aa == cc) / NUM_TESTING_IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.098000000000000004, 0.088300000000000003]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.append(eval(testimages, testlabels))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.098000000000000004]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [W1,W2,W3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[1] = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.Graph().as_default()\n",
    "    tf.device('/cpu:0')\n",
    "    sess = tf.Session()\n",
    "    lr = tf.placeholder(tf.float32)  # learning rate        \n",
    "        # hard-coded the network structure (2 hidden layers)\n",
    "    W1 = tf.Variable(weight[0], dtype=tf.float32)  # first hidden layer\n",
    "    W2 = tf.Variable(weight[1], dtype=tf.float32)  # second hidden layer\n",
    "    W3 = tf.Variable(weight[2], dtype=tf.float32)  # output layer\n",
    "    b1 = tf.Variable(bias[0], dtype=tf.float32)\n",
    "    b2 = tf.Variable(bias[1], dtype=tf.float32)\n",
    "    b3 = tf.Variable(bias[2], dtype=tf.float32)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    round = int(size / batch_size)\n",
    "        \n",
    "    for _ in xrange(epochs):\n",
    "        for i in xrange(round):\n",
    "            # sample one batch from the lot\n",
    "            image_batch = images[(i*batch_size):((i+1)*batch_size), :]\n",
    "            label_batch = labels[(i*batch_size):((i+1)*batch_size)]\n",
    "            # build the network\n",
    "            Y1 = tf.nn.relu(tf.matmul(image_batch, W1) + b1)  # first layer\n",
    "            Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)  # first layer\n",
    "            Ylogits = tf.matmul(Y2, W3) + b3  # output layer\n",
    "            Y = tf.nn.softmax(Ylogits)\n",
    "            # objective function (cross entropy)\n",
    "            cost = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=tf.one_hot(label_batch, 10))\n",
    "            cost = tf.reduce_sum(cost) / batch_size\n",
    "            op1 = tf.train.GradientDescentOptimizer(lr).compute_gradients(cost)\n",
    "            op2 = tf.train.GradientDescentOptimizer(lr).apply_gradients(op1)\n",
    "            saver = tf.train.Saver()\n",
    "            coord = tf.train.Coordinator()\n",
    "            _ = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "            sess.run(op1) \n",
    "            sess.run([op2], feed_dict={lr: learningRate})\n",
    "\n",
    "    newW1 = sess.run(W1)\n",
    "    newW2 = sess.run(W2)\n",
    "    newW3 = sess.run(W3)\n",
    "    newb1 = sess.run(b1)\n",
    "    newb2 = sess.run(b2)\n",
    "    newb3 = sess.run(b3)\n",
    "    sess.close()\n",
    "\n",
    "    # clip difference\n",
    "    W1_diff = np.clip((newW1 - weight[0]).flatten(), -bound, bound)\n",
    "    W2_diff = np.clip((newW2 - weight[1]).flatten(), -bound, bound)\n",
    "    W3_diff = np.clip((newW3 - weight[2]).flatten(), -bound, bound)\n",
    "    b1_diff = np.clip((newb1 - bias[0]).flatten(), -bound, bound)\n",
    "    b2_diff = np.clip((newb2 - bias[1]).flatten(), -bound, bound)\n",
    "    b3_diff = np.clip((newb3 - bias[2]).flatten(), -bound, bound)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images1, labels1 = MnistInput(mnist_train_file, whole=False, start=1, size = usrSize[0])\n",
    "initW1 = np.zeros((input, units[0]))\n",
    "initb1 = np.zeros(units[0])\n",
    "initW2 = np.zeros((units[0], units[1]))\n",
    "initb2 = np.zeros(units[1])\n",
    "initW3 = np.zeros((units[1], output))\n",
    "initb3 = np.zeros(output)\n",
    "\n",
    "weight = [initW1,initW2,initW3]\n",
    "bias = [initb1,initb2,initb3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tf.Graph().as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.device('/cpu:0')\n",
    "sess = tf.Session()\n",
    "lr = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = tf.Variable(weight[0], dtype=tf.float32)  # first hidden layer\n",
    "W2 = tf.Variable(weight[1], dtype=tf.float32)  # second hidden layer\n",
    "W3 = tf.Variable(weight[2], dtype=tf.float32)  # output layer\n",
    "b1 = tf.Variable(bias[0], dtype=tf.float32)\n",
    "b2 = tf.Variable(bias[1], dtype=tf.float32)\n",
    "b3 = tf.Variable(bias[2], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "round = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_batch = images1[(0*1000):((0+1)*1000)]\n",
    "label_batch = labels1[(0*1000):((0+1)*1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "all_variables_list = ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y1 = tf.nn.relu(tf.matmul(image_batch, W1) + b1)  # first layer\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)  # first layer\n",
    "Ylogits = tf.matmul(Y2, W3) + b3  # output layer\n",
    "Y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=tf.one_hot(label_batch, 10))\n",
    "cost = tf.reduce_sum(cost) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "coord = tf.train.Coordinator()\n",
    "_ = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "op1 = tf.train.GradientDescentOptimizer(lr).compute_gradients(cost, all_variables_list)\n",
    "op2 = tf.train.GradientDescentOptimizer(lr).apply_gradients(op1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print op1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(784, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1:0' shape=(100, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_2:0' shape=(50, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_3:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_4:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_5:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(op1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_1:0' shape=(10000,) dtype=int32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy.append(eval(testimages, testlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = sorted(range(len(b)), key=lambda i: abs(b[i]))[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b  = W1_diff + np.random.laplace(loc = 0.0, scale = 1, size = len(W1_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisyMax(grad, n, epsilon, bound):\n",
    "    \"\"\"\n",
    "    report noisy maximum algorithm\n",
    "            Args:\n",
    "                grad: true gradient of each parameter\n",
    "                n: # of output noisy gradient\n",
    "                scale: scale of the laplace distribution (calibrated by sensitivity)\n",
    "            Returns: \n",
    "                the noisy version of the top n gradient\n",
    "    \"\"\"\n",
    "    scale = 2 * bound *2 * n / epsilon  \n",
    "    noisyGrad = grad + np.random.laplace(loc = 0.0, scale = scale, size = len(grad))\n",
    "    index = sorted(range(len(noisyGrad)), key=lambda i: abs(noisyGrad[i]))[-n:]\n",
    "    return [noisyGrad[i] if i in index else 0 for i in range(noisyGrad)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = noisyMax(W1_diff, len(W1_diff) * .1, .5, grad_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = 2 * grad_bound *2 * len(W1_diff) * .1 / .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noisyGrad = W1_diff + np.random.laplace(loc = 0.0, scale = scale, size = len(W1_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = sorted(range(len(noisyGrad)), key=lambda i: abs(noisyGrad[i]))[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[noisyGrad[i] if i in [1,2,3,4,5] else 0 for i in range(noisyGrad)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noisyGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[testimages[1],testimages[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testimages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images1, labels1 = MnistInput(mnist_train_file, whole=False, start=1, size = usrSize[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [images1[i] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = tf.convert_to_tensor(a, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.random.normal(0, 1, (100,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight[0] = np.random.normal(0, 1, (784, 100))\n",
    "#weight[0] = np.zeros((784,100))\n",
    "weight[1] = np.random.normal(0, 1, (100, 50))\n",
    "weight[2] = np.random.normal(0, 1, (50, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight[1] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
